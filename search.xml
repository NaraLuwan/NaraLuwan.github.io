<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL ERROR 1290 错误码原因及解决办法]]></title>
    <url>%2F2019%2F02%2F13%2FmysqlLoadDataInFile%2F</url>
    <content type="text"><![CDATA[12mysql&gt; Load Data InFile 'C:/test.csv' Into Table `test` Lines Terminated By '\r\n';ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute this statement 报错原因secure_file_priv 参数设置了指定目录，只能在指定的目录下进行数据导入导出。 12345678mysql&gt; show variables like '%secure%';+--------------------------+------------------------------------------------+| Variable_name | Value |+--------------------------+------------------------------------------------+| require_secure_transport | OFF || secure_file_priv | C:\ProgramData\MySQL\MySQL Server 8.0\Uploads\ |+--------------------------+------------------------------------------------+2 rows in set, 1 warning (0.00 sec) 解决办法 可以设置 my.cnf 配置文件，增加以下配置：1secure_file_priv='/xx/xx' 需要保证 mysql 用户和当前执行命令的用户具有该目录的读写权限。 或者直接将需要导入的文件放在 secure_file_priv 指定的目录即可。导出同理。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AES + RSA + Hash 实现 C-S 安全交互]]></title>
    <url>%2F2019%2F01%2F31%2Fencrypt%2F</url>
    <content type="text"><![CDATA[概述AES 由于其执行速度快，易于硬件实现，破解难度大等优势，被广泛用于数据的加密。 既然是对称加密，那如何保证秘钥的安全传输？很容易想到用 RSA 加密秘钥。由于只能用私钥解密，而私钥不需要交互双方都知道也就不用通过网络传输，只要私钥不泄露信息就是安全的。 但如果别人截取到请求后伪造数据也用 RSA 公钥加密这种情况呢？也就是如何保证数据的准确性？这个时候就需要签名校验。 本文基于 AES + RSA + Hash 实现一套完整的足够安全的加解密算法。 流程图 实现Client 端 生成 AES 密钥。 使用生成的 AES 密钥对请求的明文数据进行加密，得到 EncryptData。 使用 Server 端提供的接口获取RSA公钥。 使用获取到的 RSA 公钥对 AES 密钥进行加密，得到 EncryptAesKey。 生成签名（CRC 或 Hash 都可以，简单点可以只对 AES 秘钥按一定的规则转换后 Hash）。 将 EncryptAesKey EncryptData 和 Hash 一起发送给 Server 端。 Server 端 生成 RSA 密钥对,并提供接口给 client 获取 RSA 公钥（或者直接私下明文约定好）。 响应 Client 的 Http 请求，获取到 EncryptAesKey EncryptData 和 Hash。 使用 RSA 私钥 EncryptAesKey 进行 RSA 解密，得到 AES 密钥 AesKey。 按照约定的规则对 AesKey 进行转换后再生成签名，校验获取到的 Hash 字段，如不通过就不用继续后边的处理了。 使用最终的 AesKey 对 EncryptData 进行 AES 解密，得到明文数据。 做响应的处理，返回结果。 注：返回结果的加解密逆推回去即可。 一些扩展 签名前最好对参与签名的字段先 Base64 编码一下，避免一些特殊字符导致签名校验不通过，返回结果最好也编码下。 请求和返回数据最好增加时间戳或 UUID 字段，这样生成的签名基本不会出现重复的情况，而且每次都有变动。]]></content>
      <categories>
        <category>加解密</category>
      </categories>
      <tags>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Base64编码原理]]></title>
    <url>%2F2019%2F01%2F30%2Fbaes64%2F</url>
    <content type="text"><![CDATA[概述 Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于每6个比特为一个单元，对应某个可打印字符。3个字节有24个比特，对应于4个Base64单元，即3个字节可由4个可打印字符来表示。它可用来作为电子邮件的传输编码。在Base64中的可打印字符包括字母A-Z、a-z、数字0-9，这样共有62个字符，此外两个可打印符号在不同的系统中而不同。 Base64常用于在通常处理文本数据的场合，表示、传输、存储一些二进制数据，包括MIME的电子邮件及XML的一些复杂数据。 概况来说，Base64 就是选出 64 个字符（小写字母a-z、大写字母A-Z、数字0-9、符号”+”、”/“），作为一个基本字符集，将其他所有符号都转换成这个字符集中的字符。 缺点：因为 Base64 将三个字节转化成四个字节，因此 Base64 编码后的文本，会比原文本大出三分之一左右。 优点：有些网关或系统只能使用ASCII字符。Base64就是用来将非ASCII字符的数据转换成ASCII字符的一种方法，而且base64特别适合在http，mime协议下快速传输数据。 步骤 将每三个字节作为一组，一共是24个二进制位。 将这24个二进制位分为四组，每个组有6个二进制位。 在每组前面加两个00，扩展成32个二进制位，即四个字节。 根据基本字符表，得到扩展后的每个字节的对应符号，这就是Base64的编码值。 实例正常情况 需要补位的情况 JDK APIJDK 1.8 版本以后 util 包里增加了 Base64 API。 12345678910111213141516171819202122static class TestBase64 &#123; public static void main(String[] args) &#123; // Basic 编码：标准的BASE64编码，用于处理常规的需求。支持 byte[] -&gt; byte[] byte[] &lt;-&gt; String String encodeHelloStr = Base64.getEncoder().encodeToString("hello".getBytes()); byte[] decodeHelloByte = Base64.getDecoder().decode(encodeHelloStr); System.out.println("hello".equals(new String(decodeHelloByte))); // true // URL 编码：使用下划线替换URL里面的反斜线 / String urlEncoded = Base64.getUrlEncoder().encodeToString("index?a=xx".getBytes()); System.out.println(urlEncoded); // MIME 编码：使用基本的字母数字产生BASE64输出，而且对MIME格式友好：每一行输出不超过76个字符，而且每行以“\r\n”符结束 StringBuilder sb = new StringBuilder(); for (int t = 0; t &lt; 10; ++t) &#123; sb.append(UUID.randomUUID().toString()); &#125; byte[] toEncode = sb.toString().getBytes(); String mimeEncoded = Base64.getMimeEncoder().encodeToString(toEncode); System.out.println(mimeEncoded); &#125; &#125; 参考Base64笔记 Java 8新特性探究（十一）Base64详解]]></content>
      <categories>
        <category>编码</category>
      </categories>
      <tags>
        <tag>cs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala 学习笔记]]></title>
    <url>%2F2019%2F01%2F12%2Fscala01%2F</url>
    <content type="text"><![CDATA[下午抽时间学习了下 scala 的基本语法，有 java 基础看起来很快，这里主要记录一下与 java 不同的地方。 scala 简介Scala 是 Scalable Language 的简写，是一门多范式的编程语言。 scala 特性 面向对象：每个值都是对象。对象的数据类型以及行为由类和特征描述。 函数式编程：函数也能当成值（对象）来使用。 静态类型：通过编译时检查，保证代码的安全性和一致性。 扩展性：可以以库的形式无缝添加新的语言结构，自动闭包等。 并发性：使用Akka作为默认 Actor 实现，可以复用线程。 数据类型与 Java 有着相同的数据类型： Byte,Char,Short,Int,Long,Float,Double,Boolean 不同的是这些数据类型在 scala 中都是类，也就是说scala没有java中的原生类型。在scala可以对数字等基础类型调用方法。 变量使用关键词 var 声明变量，使用关键词 val 声明常量。声明变量的时候可以不指定类型，编译器会根据初始值自动推断。 访问修饰符与 Java 有着相同的访问修饰符： private，protected，public。 不同的是 Scala 中的 private 限定符比 Java 更严格，在嵌套类情况下，外部类不能访问被嵌套类的私有成员。 运算符和 Java 不同的是，Scala 中算术运算符没有 ++/– 操作符，要使用 +=/-=。 条件/循环if else 使用和 java 一样。循环语句不支持 break 或 continue 语句，需要依赖 scala.util.control._ 包中的 Breaks 对象。 方法/函数方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。使用 val 语句可以定义函数，def 语句定义方法。 闭包闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量，可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。类似于 JavaScript 中的闭包。 数组/集合主要是定义数组的语法和 java 不同，以及一些专有的 api 如：concat 类/接口使用关键词 object 声明类，使用关键词 trait 声明接口（特征）。与 java 中 interface 不同的是，trait 可以定义属性和方法的实现。 在 scala 中同样是单继承多实现。 模式匹配对应 Java 里的 switch，但是写在选择器表达式之后。即： 选择器 match {备选项}。 异常处理同 java 一样，catch 子句是按次序捕捉的，越具体的异常越要靠前，如果抛出的异常不在catch字句中，该异常则无法处理，会被向上层抛出到调用者处。 不同的是 catch 子句的语法借用了模式匹配的思想来做异常的匹配，是一系列case字句。 文件I/OScala 进行文件写操作，直接用的都是 java中 java.io.File 包下的 I/O 类。进行文件读操作需要依赖 scala.io.Source 包下类及伴生对象。 练习代码Github: AwesomeScala]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 中的 null 值处理]]></title>
    <url>%2F2019%2F01%2F09%2FmysqlNull%2F</url>
    <content type="text"><![CDATA[问题今天在项目中遇到一个问题，查询数据库（MySQL）一张表数据时一直返回空列表，但数据库里是有数据的，排查后发现是由于 where 条件里有一个 not in 筛选条件，但是表数据里那个字段存在 null 值。 模拟一下场景，从 user 表中查询 user_name 不在指定列表中，只要列表中出现一个 null 值，返回总是空列表。sql 如下： 1SELECT * FROM `user` WHERE `user_name` NOT IN ('xx', NULL ,'xx'); 由于在 MySQL 中 null 代表的是一个未知的值，因而 not in 条件在筛选时所有数据都不符合，包括 user_name 字段为 null 的数据。 思考 not null 和 null 的区别？分别在什么情况下使用？ NULL means you do not have to provide a value for the field. NOT NULL means you must provide a value for the fields. For example, if you are building a table of registered users for a system, you might want to make sure the user-id is always populated with a value (i.e. NOT NULL), but the optional spouses name field, can be left empty (NULL) 引用 stackoverflow 上的一个回答。not null 意味着每条数据这个字段必须有一个确定的值，而 null 表示可以有也可以没有。 查了下 MySQL 官方文档，对 null 值是这样描述的： NULL columns require additional space in the row to record whether their values are NULL. For MyISAM tables, each NULL column takes one bit extra, rounded up to the nearest byte. Null 列需要更多的存储空间。所以存储空间上 not null 优于 null。 允许为 null 时应该注意哪些细节？ 就一条原则：给允许为 null 的字段设置默认值，避免出现于 null 值比较。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 个人年终总结]]></title>
    <url>%2F2018%2F12%2F31%2FsummaryOf2018%2F</url>
    <content type="text"><![CDATA[生活不可能像你想象的那么好，但是也不会像你想象的那么糟。人的脆弱和坚强都超乎自己的想象。 《羊脂球》莫泊桑 2018 年就这么过去了，在此记录一下。 今年我经历了一次不足为外人道的至暗时刻，那种感觉和十年前飞行员落选时别无二致，仿佛突然从万米高空狠狠地摔下来，但这一次我没有再悲观地任由自己摔下去，而是冷静地分析形势，预料最坏的结果，制定 plan B，然后保持乐观，认真过好每一天认真对待每一个人每一件事，我想这是值得欣慰的。 同样值得欣慰的还有： 离开了自己的舒适区，换了工作来了自己想待的城市，并且过得不错。 读了很多书。年初计划 25 本，读了 46 本，远超预期。 发展了一个兴趣爱好：钓鱼。很享受周末钓鱼的闲暇时光。 在 deadline 之前拿到了驾照，着实不易。 用第一张房票挤上了末班车（貌似成了接盘侠？） 在贸易战前夕冲进了股市，由于没钱补充子弹到现在战绩还不算很难看，这十个月的熊市让我学到了很多，关于投资、关于人性、关于自己。 列了这么多，看来我的 2018 也不是很惨，感恩 感谢。 忘了在哪里看到一张很有意思的图： 感觉自己还在绝望之谷挣扎，希望能早日启程开悟之坡。 关于 2018 就扯这么多，计划中的 2019 年： 提升技术。多向身边优秀的大牛学习，不要给人感觉还只是个 if else 工程师。 保持身心健康。泡健身房的频率要提高并稳定在 3-4 次/周。 带爸妈一块出去旅游一次，最好是北京。 多读书，60 本。如果能远超预期就把我的 Kindle 换成 paperwhite 3。 发展一个兴趣爱好。我希望是摄影。 慢慢觉得，人活着，很容易把每一天过成一个样子，我不喜欢这样，我喜欢总是有些变化。很庆幸现在的自己还是对新鲜事物有足够的好奇心和热情，新的一年继续保持吧!]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>trifle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo 架构分析]]></title>
    <url>%2F2018%2F12%2F26%2FapolloArchitecture%2F</url>
    <content type="text"><![CDATA[基础模型 用户在配置中心对配置进行修改并发布 配置中心通知 Apollo 客户端有配置更新 Apollo 客户端从配置中心拉取最新的配置、更新本地配置并通知到应用 模块分析下图是 Apollo 的七个模块，其中四个模块是和功能相关的核心模块，另外三个模块是辅助服务发现的模块： Client Apollo 客户端，为应用获取配置，实时更新 通过 Meta Server 获取 Config Service 服务列表（IP+Port），通过 IP+Port 访问服务 在 Client 侧做 load balance、错误重试 Portal 提供配置的管理后台界面 通过 Meta Server 获取 Admin Service 服务列表（IP+Port），通过 IP+Port 访问服务 在 Portal 侧做load balance、错误重试 Config Service 给客户端（client）提供配置获取和更新推送接口（基于 Http long polling） admin service 给管理后台（portal） 提供配置管理接口（Http Restful API） Meta Server 封装服务发现的细节，只是一个逻辑角色 Eureka 基于 Eureka 和 Spring Cloud Netflix 提供服务注册和发现, Config Service 和 Admin Service 会向 Eureka 注册服务，并保持心跳 为了简单起见，目前 Eureka 在部署时和 Config Service 是在一个 JVM 进程中的（通过 Spring Cloud Netflix） SLB 和域名系统配合，负载均衡 思考 封装 meta Server 的好处？ 对 Portal 和 Client 而言，永远通过一个Http接口获取 Admin Service 和 Config Service 的服务信息 由于原生 Eureka 只支持 Java 客户端，封装 meta Server 可以将 Eureka 的服务发现接口以更简单明确的HTTP接口的形式暴露出来，方便其他语言接入 为什么选用 Eureka? 它提供了完整的 Service Registry 和 Service Discovery 实现，也经受住了 Netflix 自己的生产环境考验，相对使用起来会比较省心 和 Spring Cloud 无缝集成，为了提高配置中心的可用性和降低部署复杂度，需要尽可能地减少外部依赖 由于代码是开源的，非常便于我们了解它的实现原理和排查问题 参考资料 Apollo 官方文档 Apollo 开发指南 芋道源码]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[热门分布式配置中心项目对比]]></title>
    <url>%2F2018%2F12%2F24%2FcompareConfig%2F</url>
    <content type="text"><![CDATA[主要对比Spring Cloud Config、netflix archaius、Apollo、Disconf四个项目的特性]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Apollo</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join和countDownLatch原理及区别详解]]></title>
    <url>%2F2018%2F08%2F25%2FjoinAndCountDownLatch%2F</url>
    <content type="text"><![CDATA[源码解析 join 原理：在当前线程中调用另一个线程线程 thread 的 join() 方法时，会调用该 thread 的 wait() 方法，直到这个 thread 执行完毕(JVM在 run() 方法执行完后调用 exit() 方法，而 exit() 方法里调用了 notifyAll() 方法)会调用 notifyAll() 方法主动唤醒当前线程。 源码如下：12345678910111213141516171819202122232425262728293031323334353637public final void join() throws InterruptedException &#123; join(0); &#125; /** * 注意这个方法是同步的 */ public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException("timeout value is negative"); &#125; /** * join方法默认参数为0，会直接阻塞当前线程 */ if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; public final native boolean isAlive();&#125; countDownLatch 原理：可以理解为一个计数器。在初始化 CountDownLatch 的时候会在类的内部初始化一个int的变量，每当调用 countDownt() 方法的时候这个变量的值减1，而 await() 方法就是去判断这个变量的值是否为0，是则表示所有的操作都已经完成，否则继续等待。 源码如下（源码比较少，直接全贴出来了，所有中文注释是我自己加上去的）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public static class CountDownLatch &#123; private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; /** * 初始化state */ Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; /** * 尝试获取同步状态 * 只有当同步状态为0的时候返回1 */ protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1; &#125; /** * 自旋+CAS的方式释放同步状态 */ protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125; &#125; private final Sync sync; /** * 初始化一个同步器 */ public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException("count &lt; 0"); this.sync = new Sync(count); &#125; /** * 调用同步器的acquireSharedInterruptibly方法，并且是响应中断的 */ public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1); &#125; /** * 调用同步器的releaseShared方法去让state减1 */ public void countDown() &#123; sync.releaseShared(1); &#125; /** * 获取剩余的count */ public long getCount() &#123; return sync.getCount(); &#125; public String toString() &#123; return super.toString() + "[Count = " + sync.getCount() + "]"; &#125; &#125; 区别及注意事项 join和countDownLatch都能实现让当前线程阻塞等待其他线程执行完毕，join使用起来更简便，不过countDownLatch粒度更细。 由于CountDownLatch需要开发人员很明确需要等待的条件，否则容易造成await()方法一直阻塞。 如何使用 一个简单的小例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class Test &#123; private static final Logger logger = LoggerFactory.getLogger(Test.class); public static void main(String[] args) &#123; long sleepTime = 5000; try &#123; TestJoinThread joinThread1 = new TestJoinThread(&quot;joinThread1&quot;,sleepTime); TestJoinThread joinThrad2 = new TestJoinThread(&quot;joinThrad2&quot;,sleepTime); joinThread1.start(); joinThrad2.start(); joinThread1.join(); joinThrad2.join(); logger.info(&quot;主线程开始运行...&quot;); &#125; catch (InterruptedException e) &#123; logger.error(&quot;test join err!&quot;,e); &#125; try &#123; CountDownLatch count = new CountDownLatch(2); TestCountDownLatchThread countDownLatchThread1 = new TestCountDownLatchThread(count,&quot;countDownLatchThread1&quot;,sleepTime); TestCountDownLatchThread countDownLatchThread2 = new TestCountDownLatchThread(count,&quot;countDownLatchThread2&quot;,sleepTime); countDownLatchThread1.start(); countDownLatchThread2.start(); count.await(); logger.info(&quot;主线程开始运行...&quot;); &#125; catch (InterruptedException e) &#123; logger.error(&quot;test countDownLatch err!&quot;,e); &#125; &#125; static class TestJoinThread extends Thread&#123; private String threadName; private long sleepTime; public TestJoinThread(String threadName,long sleepTime)&#123; this.threadName = threadName; this.sleepTime = sleepTime; &#125; @Override public void run() &#123; try&#123; logger.info(String.format(&quot;线程[%s]开始运行...&quot;,threadName)); Thread.sleep(sleepTime); logger.info(String.format(&quot;线程[%s]运行结束 耗时[%s]s&quot;,threadName,sleepTime/1000)); &#125;catch (Exception e)&#123; logger.error(&quot;TestJoinThread run err!&quot;,e); &#125; &#125; &#125; static class TestCountDownLatchThread extends Thread&#123; private String threadName; private long sleepTime; private CountDownLatch countDownLatch; public TestCountDownLatchThread(CountDownLatch countDownLatch,String threadName,long sleepTime)&#123; this.countDownLatch = countDownLatch; this.threadName = threadName; this.sleepTime = sleepTime; &#125; @Override public void run() &#123; try&#123; logger.info(String.format(&quot;线程[%s]开始运行...&quot;,threadName)); Thread.sleep(sleepTime); logger.info(String.format(&quot;线程[%s]运行结束 耗时[%s]s&quot;,threadName,sleepTime/1000)); countDownLatch.countDown(); &#125;catch (Exception e)&#123; logger.error(&quot;TestCountDownLatchThread run err!&quot;,e); &#125; &#125; &#125;&#125; 日志输出：1234567891011:18:01.985 [Thread-1] INFO com.sync.Test - 线程[joinThrad2]开始运行...11:18:01.985 [Thread-0] INFO com.sync.Test - 线程[joinThread1]开始运行...11:18:06.993 [Thread-1] INFO com.sync.Test - 线程[joinThrad2]运行结束...耗时[5]s11:18:06.993 [Thread-0] INFO com.sync.Test - 线程[joinThread1]运行结束...耗时[5]s11:18:06.993 [main] INFO com.sync.Test - 主线程开始运行...11:18:06.995 [Thread-2] INFO com.sync.Test - 线程[countDownLatchThread1]开始运行...11:18:06.995 [Thread-3] INFO com.sync.Test - 线程[countDownLatchThread2]开始运行...11:18:11.996 [Thread-2] INFO com.sync.Test - 线程[countDownLatchThread1]运行结束...耗时[5]s11:18:11.996 [Thread-3] INFO com.sync.Test - 线程[countDownLatchThread2]运行结束...耗时[5]s11:18:11.996 [main] INFO com.sync.Test - 主线程开始运行... 可以看到：joinThread1 和 joinThread2 同时开始执行，5s后主线程开始执行。countDownLatchThread1 和 countDownLatchThread2 也是一样的效果。 那么我上面所说的粒度更细有怎样的应用场景呢？ 我对 TestCountDownLatchThread类 的 run() 方法做一点小改动：1234567891011121314@Override public void run() &#123; try&#123; logger.info(String.format(&quot;线程[%s]第一阶段开始运行...&quot;,threadName); Thread.sleep(sleepTime); logger.info(String.format(&quot;线程[%s]第一阶段运行结束耗时[%s]s&quot;,threadName,sleepTime/1000)); countDownLatch.countDown(); logger.info(String.format(&quot;线程[%s]第二阶段开始运行...&quot;,threadName); Thread.sleep(sleepTime); logger.info(String.format(&quot;线程[%s]第二阶段运行结束耗时[%s]s&quot;,threadName,sleepTime/1000)); &#125;catch (Exception e)&#123; logger.error(&quot;TestCountDownLatchThread run err!&quot;,e); &#125;&#125; 这个时候日志输出会变成这样：12345678912:59:35.912 [Thread-1] INFO com.sync.Test - 线程[countDownLatchThread2]第一阶段开始运行...12:59:35.912 [Thread-0] INFO com.sync.Test - 线程[countDownLatchThread1]第一阶段开始运行...12:59:40.916 [Thread-0] INFO com.sync.Test - 线程[countDownLatchThread1]第一阶段运行结束 耗时[5]s12:59:40.916 [Thread-1] INFO com.sync.Test - 线程[countDownLatchThread2]第一阶段运行结束 耗时[5]s12:59:40.916 [main] INFO com.sync.Test - 主线程开始运行...12:59:40.916 [Thread-0] INFO com.sync.Test - 线程[countDownLatchThread1]第二阶段开始运行...12:59:40.916 [Thread-1] INFO com.sync.Test - 线程[countDownLatchThread2]第二阶段开始运行...12:59:45.917 [Thread-0] INFO com.sync.Test - 线程[countDownLatchThread1]第二阶段运行结束 耗时[5]s12:59:45.917 [Thread-1] INFO com.sync.Test - 线程[countDownLatchThread2]第二阶段运行结束 耗时[5]s 也就是说如果当前线程只需要等待其他线程一部分任务执行完毕的情况下就可以用 countDownLatch 来实现了，而 join 则实现不了这种粒度的控制。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FIle类常用工具方法整理（持续更新）]]></title>
    <url>%2F2018%2F08%2F18%2FfileTools%2F</url>
    <content type="text"><![CDATA[整理记录工作中经常用到的文件相关的工具方法。 递归遍历一个目录，获取所有文件名（也可以取到绝对路径） 12345678910111213141516171819202122232425public static void traverse(String filePath, List&lt;String&gt; files) &#123; if (StringUtils.isBlank(filePath))&#123; return ; &#125; try&#123; File superFile = new File(filePath); if (superFile.exists()) &#123; File[] fileList = superFile.listFiles(); if (null != files &amp;&amp; fileList.length &gt; 0) &#123; for (File file : fileList) &#123; // 还是文件夹 if (file.isDirectory()) &#123; traverse(file.getAbsolutePath(),files); &#125; else &#123; files.add(file.getName()); //文件名 //files.add(file.getAbsolutePath()); //文件绝对路径 &#125; &#125; &#125; &#125; &#125;catch (Exception e)&#123; //log &#125; return ; &#125; 获取文件大小，自动用K、M、G表示 1234567891011121314151617public static String parseSize(long length)&#123; StringBuilder size = new StringBuilder(); DecimalFormat format = new DecimalFormat("###.0"); if (length &lt; 1024) &#123; size.append((int) length).append(" B"); &#125;else if (length &gt;= 1024 &amp;&amp; length &lt; 1024 * 1024) &#123; double i = (length / (1024.0)); size.append(format.format(i)).append(" K"); &#125;else if (length &gt;= 1024 * 1024 &amp;&amp; length &lt; 1024 * 1024 * 1024) &#123; double i = (length / (1024.0 * 1024.0)); size.append(format.format(i)).append(" M"); &#125;else if (length &gt;= 1024 * 1024 * 1024) &#123; double i = (length / (1024.0 * 1024.0 * 1024.0)); size.append(format.format(i)).append(" G"); &#125; return size.toString(); &#125; Multipart文件转存为本地的File 123456789101112131415161718public static void multipartToFile(MultipartFile file, String fileFolder)&#123; FileOutputStream outputStream = null; try &#123; File newFileFolder = new File(fileFolder); if (!newFileFolder.exists()) &#123; newFileFolder.mkdirs(); &#125; fileFolder = newFileFolder.getAbsolutePath() + File.separator + file.getOriginalFilename(); outputStream = new FileOutputStream(new File(fileFolder)); IOUtils.copy(file.getInputStream(), outputStream); &#125; catch (Exception e) &#123; // log &#125; finally &#123; IOUtils.closeQuietly(outputStream); &#125; &#125; 清理指定目录下一天前（时间可以指定）的文件 123456789101112131415161718192021public static void cleanDirectory(String dir, long ttl) &#123; File file = new File(dir); String[] subDirNames = file.list(new FilenameFilter() &#123; @Override public boolean accept(File current, String name) &#123; return new File(current, name).isDirectory(); &#125; &#125;); if (subDirNames != null) &#123; for (String name : subDirNames) &#123; File subDir = new File(dir + File.separator + name); if (System.currentTimeMillis() - subDir.lastModified() &gt; ttl) &#123; try &#123; FileUtils.deleteDirectory(subDir); //import org.apache.commons.io.FileUtils; &#125; catch (Exception e) &#123; // log &#125; &#125; &#125; &#125; &#125; 把字符串存入指定文件 123456789101112131415161718public static void strToFile(String content, File outFile) &#123; OutputStream os = null; try &#123; File parent = outFile.getParentFile(); if (!parent.exists()) &#123; parent.mkdirs(); &#125; if (!outFile.exists()) &#123; outFile.createNewFile(); &#125; os = new FileOutputStream(outFile); IOUtils.write(content, os); &#125; catch (Exception e) &#123; // log &#125; finally &#123; IOUtils.closeQuietly(os); &#125; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>File</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL主从原理及实践]]></title>
    <url>%2F2018%2F06%2F21%2FmysqlMasterToSlave%2F</url>
    <content type="text"><![CDATA[原理概述利用MySQL提供的Replication，其实就是Slave从Master获取Binary log文件，然后再本地镜像的执行日志中记录的操作。由于主从复制的过程是异步的，因此Slave和Master之间的数据有可能存在延迟的现象，此时只能保证数据最终的一致性。 在master与slave之间实现整个复制过程主要由三个线程来完成： Slave SQL thread线程，在slave端 Slave I/O thread线程，在slave端 Binlog dump thread线程(也可称为IO线程)，在master端 注意：如果一台主服务器配两台从服务器那主服务器上就会有两个Binlog dump 线程，而每个从服务器上各自有两个线程。 网络上找的一张很清晰的交互图，流程可以详细描述为： slave端的IO线程连接上master端，并请求从指定binlog日志文件的指定pos节点位置(或者从最开始的日志)开始复制之后的日志内容。 master端在接收到来自slave端的IO线程请求后，通知负责复制进程的IO线程，根据slave端IO线程的请求信息，读取指定binlog日志指定pos节点位置之后的日志信息，然后返回给slave端的IO线程。该返回信息中除了binlog日志所包含的信息之外，还包括本次返回的信息在master端的binlog文件名以及在该binlog日志中的pos节点位置。 slave端的IO线程在接收到master端IO返回的信息后，将接收到的binlog日志内容依次写入到slave端的relaylog文件(mysql-relay-bin.xxxxxx)的最末端，并将读取到的master端的binlog文件名和pos节点位置记录到master-info（该文件存slave端）文件中，以便在下一次读取的时候能够清楚的告诉master“我需要从哪个binlog文件的哪个pos节点位置开始，请把此节点以后的日志内容发给我”。 slave端的SQL线程在检测到relaylog文件中新增内容后，会马上解析该log文件中的内容。然后还原成在master端真实执行的那些SQL语句，并在自身按顺丰依次执行这些SQL语句。这样，实际上就是在master端和slave端执行了同样的SQL语句，所以master端和slave端的数据完全一样的。 如何实现这里在本地虚拟机模拟演示： 准备工作两个虚拟机：我这里用的是CentOS5.5，IP地址分别是192.168.1.101 和192.168.1.105； 步骤： 修改mysql的配置文件，打开二进制日志功能。 命令：# vi /etc/my.cnf 增加以下三行配置 log-bin=mysql-bin //将mysql二进制日志取名为mysql-binbinlog_format=mixed //二进制日志的格式，有三种：statement/row/mixed,具体分别不多做解释，这里使用mixedserver-id=101 //为服务器设置一个独一无二的id便于区分，这里使用ip地址的最后一位充当server-id然后修改从服务器的my.cnf文件，也添加以上内容，不过server-id的值为从服务器的标识。然后分别重启主、从mysql。 在主服务器为从服务器分配用户。命令：GRANT replication slave ON . TO ‘slave‘@’%’ IDENTIFIED BY ‘111111’; 查看主服务器BIN日志的信息（执行完之后记录下这两值，然后在配置完从服务器之前不要对主服务器进行任何操作，因为每次操作数据库时这两值会发生改变）。命令：show master status; 进入从服务器，执行以下命令设置master。 MASTER_HOST : 设置要连接的主服务器的ip地址MASTER_USER : 设置要连接的主服务器的用户名MASTER_PASSWORD : 设置要连接的主服务器的密码MASTER_LOG_FILE : 设置要连接的主服务器的bin日志的日志名称，即第3步得到的信息MASTER_LOG_POS : 设置要连接的主服务器的bin日志的记录位置，即第3步得到的信息，（这里注意，最后一项不需要加引号。否则配置失败）然后执行start slave;命令启动从服务器。 查看是否配置成功。命令：show slave status; 上面两项为Yes说明配置成功。可以在主库更新数据测试从库是否会同步。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring事务管理源码解析--加了@Transactional注解后Spring究竟为我们做了哪些事情？]]></title>
    <url>%2F2018%2F04%2F19%2FspringTranscational%2F</url>
    <content type="text"><![CDATA[实现一个事务需要以下几步：1.获取数据库连接 2.执行数据库操作 3.如果2步骤发生异常就回滚，否则就提交 4.释放资源。 1、3、4步骤是所有事务所共有的逻辑，程序真正需要关心的只有第2步，spring的事务管理也正是帮我们做了1、3、4的工作。 准备工作 第一步，要配置dataSource、transactionManager 123456789101112131415&lt;!--启动spring注解功能 --&gt;&lt;tx:annotation-driven transactionmanager="transactionManager" /&gt;&lt;!-- 设定transactionManager --&gt;&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt;&lt;/bean&gt;&lt;!-- 配置数据源，这里以C3P0为例 --&gt;&lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource" destroy-method="close"&gt; &lt;property name="driverClass" value="..."&gt;&lt;/property&gt; ... &lt;property name="checkoutTimeout" value="..."/&gt;&lt;/bean&gt; 第二步，在需要事务的方法上加上注解 @Transactional 即可。 配置参数源码位置：org.springframework.transaction.annotation.Transactional12345678910111213141516171819202122232425262728293031323334@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Transactional &#123; @AliasFor("transactionManager") String value() default ""; @AliasFor("value") String transactionManager() default ""; // 事务的传播属性，默认已有事务存在就加入当前事务，否则就开启一个新事务 Propagation propagation() default Propagation.REQUIRED; // 隔离级别，默认当前所用数据库的隔离级别，下面以Mysql为例 Isolation isolation() default Isolation.DEFAULT; // 超时时间 int timeout() default TransactionDefinition.TIMEOUT_DEFAULT; // 是否只读 boolean readOnly() default false; // 发生哪些异常时回滚事务 Class&lt;? extends Throwable&gt;[] rollbackFor() default &#123;&#125;; String[] rollbackForClassName() default &#123;&#125;; // 发生哪些异常时不回滚事务 Class&lt;? extends Throwable&gt;[] noRollbackFor() default &#123;&#125;; String[] noRollbackForClassName() default &#123;&#125;;&#125; 这里主要关注以下三个参数： 传播属性 隔离级别 超时时间 前面配置的transactionManager（即DataSourceTransactionManager）继承关系图如下： 我们只看PlatformTransactionManager–&gt;AbstractPlatformTransactionManager–&gt;DataSourceTransactionManager这条线，先来看PlatformTransactionManager接口： 源码位置：org.springframework.transaction.PlatformTransactionManager123456789public interface PlatformTransactionManager &#123; TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;&#125; 它是spring事务管理器的顶层接口，这三个方法是对应前边所说的1、3两个步骤，分别为开启事务、提交、回滚。再来看它的实现类AbstractPlatformTransactionManager，主要看上边三个方法对应的子类实现，可以发现它们都是一样的套路：先做一些边界条件判断，然后调用自己内部的对应do…方法，而且每个对应的do…方法都是protected abstract 修饰的，实际上继承于AbstractPlatformTransactionManager 的类有很多，如DataSourceTransactionManager , JtaTransactionManager, HibernateTransactionManager 等，不管是哪一个种，总要有 getTransaction，commit，rollback的方法，所以这个三个方法在接口中，不过具体实现方式有所区别，所以具体的实现需要而且是必须在子类中完成。AbstractPlatformTransactionManager 规定了这三个方法的调用顺序，真正的细节在子类中以”do*”开头的方法中实现，这也正是大多数抽象类的作用所在。 我们先来看此抽象类的 getTransaction 方法： 重点关注我标注出来的两步，先调用doGetTransaction方法获取一个事务（新开启的或者已经存在的），然后调用doBegin方法开始执行事务。 我们直接去看具体实现类的这两个方法，这里以DataSourceTransactionManager为例。doGetTransaction方法其实就是返回我们配置的全局数据源。 再来看doBegin方法，先获取一个connetion，设置autoCommit为false，设置隔离级别… 然后调用 service 中的代码，如果没有抛出异常Spring框架将继续调用AbstractPlatformTransactionManager中的commit方法，继续看源码： 还是要看子类doCommit方法，先拿到连接（其实就是之前获取到的那个），然后commit，有异常就抛给上层。 这里先抛出两个问题：spring事务管理如何保证同一个线程总是能获取到已开启的事务？也就是说默认传播属性下同一个线程怎么保证最多只会有一个事务？ 现在事务已经提交了，如果发生异常的话，父类中调用方法 rollback方法： 继续看子类doRollback方法： 现在回想我刚才抛出的两个问题，不知道你有没有发现，在提交和回滚的是获取当前连接都是这样调用： 1Connection con = txObject.getConnectionHolder().getConnection(); 那怎么保证每次get的是同一个连接？ 来看的dobegin方法，重点关注bind the session holder这个方法 附上resource的定义，可以发现它是一个ThreadLocal。 1private static final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; resources = new NamedThreadLocal&lt;Map&lt;Object, Object&gt;&gt;("Transactional resources"); 现在明白了吧，spring在开启事务的时候会把获取到的连接绑定在事务的上下文环境中，然后把这个上下文环境又绑定在当前线程的ThreadLocal中，这样每个线程的事务就是独立的了，那同一个线程每次获取连接获取到的也就是同一个了。 最后看一下对资源释放的代码，父类中cleanupAfterCompletion方法调用了doCleanupAfterCompletion，我们直接看子类实现： releaseConnection方法就是进行资源释放的工作，逻辑比较简单就不分析了。 至此 Spring 帮我们管理的事务，其主要流程和方法已经介绍完了，当然其中还有很多 private 方法和 protected 方法没有介绍。我想只要把主干捋清楚之后，其他一些方法也很好理解了。 绕了一大圈，实际上还是绕不过开启事务，提交事务，回滚事务三件事，只是这三件事情现在由Spring帮助我们在背后默默做好了。]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 磁盘管理]]></title>
    <url>%2F2018%2F02%2F18%2Flinux05%2F</url>
    <content type="text"><![CDATA[常用命令 df：检查文件系统的磁盘空间占用情况。 1语法：df [-ahikHTm] 目录或文件名 参数：-a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统；-k ：以 KBytes 的容量显示各文件系统；-m ：以 MBytes 的容量显示各文件系统；-h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；-H ：以 M=1000K 取代 M=1024K 的进位方式；-T ：显示文件系统类型, 连同该 partition 的 filesystem 名称 (例如 ext3) 也列出；-i ：不用硬盘容量，而以 inode 的数量来显示 du：是对文件和目录磁盘使用的空间的查看 1语法：du [-ahskm] 文件或目录名称 参数：-a ：列出所有的文件与目录容量，因为默认仅统计目录底下的文件量而已。-h ：以人们较易读的容量格式 (G/M) 显示；-s ：列出总量而已，而不列出每个各别的目录占用容量；-S ：不包括子目录下的总计，与 -s 有点差别。-k ：以 KBytes 列出容量显示；-m ：以 MBytes 列出容量显示； fdisk：是 Linux 的磁盘分区表操作工具 1语法：fdisk [-l] 装置名称 参数：-l ：输出后面接的装置所有的分区内容。若仅有 fdisk -l 时， 则系统将会把整个系统内能够搜寻到的装置的分区均列出来。 mkfs：磁盘格式化1语法：mkfs [-t 文件系统格式] 装置文件名 参数：-t ：可以接文件系统格式，例如 ext3, ext2, vfat 等(系统有支持才会生效) mount：磁盘挂载与卸除 1语法：mount [-t 文件系统] [-L Label名] [-o 额外选项] [-n] 装置文件名 挂载点]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 用户和用户组管理]]></title>
    <url>%2F2018%2F02%2F18%2Flinux04%2F</url>
    <content type="text"><![CDATA[Todo用户账号的添加、删除与修改。 用户口令的管理。 用户组的管理。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 文件系统]]></title>
    <url>%2F2018%2F02%2F18%2Flinux03%2F</url>
    <content type="text"><![CDATA[文件基本属性ll或者ls –l命令来显示一个文件的属性以及文件所属的用户和组： 123[root@localhost ~] ls -ltotal 0drwxrwxr-x. 2 root root 6 Feb 19 20:30 test 第0位确定文件类型，第1-3位确定属主（该文件的所有者）拥有该文件的权限。 第4-6位确定属组（所有者的同组用户）拥有该文件的权限，第7-9位确定其他用户拥有该文件的权限。 文件与目录管理处理目录的常用命令ls: 列出目录cd：切换目录pwd：显示目前的目录mkdir：创建一个新的目录rmdir：删除一个空的目录cp: 复制文件或目录rm: 移除文件或目录mv: 移动文件与目录，或修改文件与目录的名称 查看文件内容的常用命令cat 由第一行开始显示文件内容tac 从最后一行开始显示，可以看出 tac 是 cat 的倒著写！nl 显示的时候，顺道输出行号！more 一页一页的显示文件内容less 与 more 类似，但是比 more 更好的是，他可以往前翻页！head 只看头几行tail 只看结尾几行]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 系统目录结构]]></title>
    <url>%2F2018%2F02%2F18%2Flinux02%2F</url>
    <content type="text"><![CDATA[默认目录 /bin：bin 是 Binary 的缩写, 这个目录存放着经常使用的命令。 /boot：这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ：dev 是 Device(设备)的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。 /etc：这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media：linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux 会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt：这是给主机额外安装软件的目录。比如你安装一个 ORACLE 数据库则就可以放到这个目录下。默认是空的。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： 1/proc/sys/net/ipv4/icmp_echo_ignore_all /root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s就是 Super User 的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux：这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放 selinux 相关的文件的。 /srv：该目录存放一些服务启动之后需要提取的数据。 /sys：这是 linux2.6 内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。 sysfs 文件系统集成了下面3种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的devpts文件系统。 该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp：这个目录是用来存放一些临时文件的。 /usr：这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 重要目录在 Linux 系统中，下列几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件： /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在 /bin/ls 目录下的。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 基础]]></title>
    <url>%2F2018%2F02%2F18%2Flinux01%2F</url>
    <content type="text"><![CDATA[Linux 简介Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。 发行版Linux 的发行版说简单点就是将 Linux 内核与应用软件做一个打包。 目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。 与 Unix 区别 Linux 是开发源代码的自由软件，而 Unix 是对源代码实行知识产权保护的传统商业软件。 Unix 系统大多是与硬件配套的，而 Linux 则可运行在多种硬件平台上。 Unix 的历史久于 Linux，Linux 的思想源于 Unix。 在功能上，Linux 没有 Unix 那么全面。 Linux 系统启动 内核的引导。 首先是BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动。操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行 init。 init 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。 init 程序首先是需要读取配置文件 /etc/inittab。 init程序的类型SysV: init, CentOS 5之前, 配置文件： /etc/inittab。Upstart: init,CentOS 6, 配置文件： /etc/inittab, /etc/init/*.conf。Systemd： systemd, CentOS 7,配置文件： /usr/lib/systemd/system、 /etc/systemd/system。 运行级别许多程序需要开机启动。它们在 Windows 叫做”服务”（service），在 Linux 就叫做”守护进程”（daemon）。 init 进程的一大任务，就是去运行这些开机启动的程序。 但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动 Apache，用作桌面就不需要。 Linux 允许为不同的场合，分配不同的开机启动程序，这就叫做”运行级别”（runlevel）。也就是说，启动时根据”运行级别”，确定要运行哪些程序。 系统初始化。 在 init 的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit 它调用执行了/etc/rc.d/rc.sysinit，而rc.sysinit是一个 bash shell 的脚本，它主要是完成一些系统初始化的工作，rc.sysinit 是每一个运行级别都要首先运行的重要脚本。 它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。 1l5:5:wait:/etc/rc.d/rc 5 这一行表示以5为参数运行/etc/rc.d/rc，/etc/rc.d/rc是一个Shell脚本，它接受5作为参数，去执行/etc/rc.d/rc5.d/目录下的所有的rc启动脚本，/etc/rc.d/rc5.d/目录中的这些启动脚本实际上都是一些连接文件，而不是真正的rc启动脚本，真正的rc启动脚本实际上都是放在/etc/rc.d/init.d/目录下。 而这些 rc 启动脚本有着类似的用法，它们一般能接受 start、stop、restart、status 等参数。 /etc/rc.d/rc5.d/ 中的 rc 启动脚本通常是K或S开头的连接文件，对于以 S 开头的启动脚本，将以 start 参数来运行。 而如果发现存在相应的脚本也存在K打头的连接，而且已经处于运行态了(以/var/lock/subsys/下的文件作为标志)，则将首先以 stop 为参数停止这些已经启动了的守护进程，然后再重新运行。 这样做是为了保证是当 init 改变运行级别时，所有相关的守护进程都将重启。 至于在每个运行级中将运行哪些守护进程，用户可以通过 chkconfig 或 setup 中的 “System Services” 来自行设定。 建立终端。 rc 执行完毕后，返回 init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。 init 接下来会打开6个终端，以便用户登录系统。 用户登录系统。 一般来说，用户的登录方式有三种： 命令行登录 ssh 登录 图形界面登录 Linux 的账号验证程序是 login，login 会接收 mingetty 传来的用户名作为用户名参数。 然后 login 会对用户名进行分析：如果用户名不是 root，且存在 /etc/nologin 文件，login 将输出 nologin 文件的内容，然后退出。 这通常用来系统维护时防止非root用户登录。只有/etc/securetty中登记了的终端才允许 root 用户登录，如果不存在这个文件，则 root 用户可以在任何终端上登录。 /etc/usertty文件用于对用户作出附加访问限制，如果不存在这个文件，则没有其他限制。 Linux 系统关闭正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt sync 将数据由内存同步到硬盘中。 shutdown 关机指令。eg:shutdown –h +10 十分钟后关机 reboot 就是重启，等同于 shutdown –r now halt 关闭系统，等同于shutdown –h now 和 poweroff]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网易社招面经，纯干货分享]]></title>
    <url>%2F2018%2F01%2F02%2Fnetease%2F</url>
    <content type="text"><![CDATA[个人背景本人毕业于二流一本大学非计算机相关专业，大三下学期开始学java。目前刚好工作两年，专业后端，base深圳。 面试流程一面二面电话面–&gt;三面四面视频面–&gt;主管电话面–&gt;hr电话面整个流程下来就两个礼拜，比阿里高效多了。由于面试的是网易杭州研究院的职位，本来三面通过后hr联系我说是要去杭州总部让主管进行个现场面试，而我在深圳，就告诉hr我这边不方便看能不能安排视频面试，然后就又加了一轮技术视频面。顺带说一句，网易的hr真是超级nice啊，加了微信私下聊得很嗨皮～ 重点：面试题java基础 定义Integer x=20 Integer y=200 在内存里是个什么过程？ volite关键字的原理？它能保证原子性吗？AtomicInteger底层怎么实现的？ threadLocal关键字有用过吗？如果没有重写initialValue方法就直接get会怎样？ hashMap与concurrentHashMap原理和区别？hashMap什么情况下会出现循环链表？concurrentHashMap写的时候用什么锁？RenteenLock底层是怎么保证线程安全的？ 反射能获取到父类的私有方法吗？怎么防止反射破坏单例模式？ 描述下JVM内存模型。每个区的作用是什么？堆内存的工作原理，为什么需要两个幸存区？只有一个行不行？老生代是用什么垃圾回收算法？ 描述下多线程原理。怎么开启一个线程？start和run方法有什么区别？怎么创建一个线程池，传入的参数分别什么含义？线程池是怎么实现维持核心线程数的？怎么实现一个自定义的拒绝策略？ Socket编程 nio（这一块我不太熟就说没了解过，面试官就没细问了） 开源框架1.你用过哪些开源框架？最熟悉的是哪个？（这里我说了spring，所以后边的问题都是围绕spring的） 2.描述下spring的ioc和aop。 你常用哪一种注入方式？BeanFactory和ApplicationContext有什么区别？你们项目里用的哪个？说一下spring bean的生命周期。 AOP实现原理是什么？两种动态代理实现原理？JDK动态代理为什么要实现接口？ 3.spring task是怎么实现的？ 4.spring事务你是怎么用的？加了@Transcational注解spring都做了哪些工作？怎么知道事务执行成功了？ 事务隔离级别？mysql默认级别是什么？事务传播属性？spring默认是什么？嵌套事务子事务什么时候commit？ 5.spring和springMVC是什么关系？有没有用过JdbcTemplate？ 6.springMVC中对整个请求的处理流程是怎样的？返回json的话是用哪个view？ 数据库1.mysql索引是怎么实现的？b+树有哪些特点？真实的数据存在哪里？ 2.哪些情况下建索引？解释下最左匹配原则？ 现在一个表有三列a b c，组合索引(a,b,c)查询的时候where a like ? and b=? and c=?能用到这个组合索引吗？为什么？ 3.explain执行计划看过没有？其中type字段都有哪些值？分别代表什么？ 4.你有哪些sql调优经验？ Redis1.redis有哪几种数据结构？给你一个key怎么知道是用的哪种结构？ 2.怎么查看所有的key？redis怎么切换库？怎么清数据？ 3.描述下redis淘汰策略？如果没有数据可以淘汰活着没有配置淘汰策略读请求可以正常执行吗？ 4.你们项目里redis是单节点的吗？如果多节点怎么同步？ 5.项目里用redis存哪些数据？为什么用redis？和jetty本地缓存有什么区别？ 网络1.HTTP 1.1版本增加了哪些内容？有哪几种请求方式？ 2.描述下HTTP三次握手和四次挥手过程？为什么需要四次挥手？为什么TIME_WAIT状态需要经过两个最大报文段生存时间才能到close状态？ 3.浏览器发起一个请求到收到响应中间经历了哪些过程？知道多少就说多少，越详细越好。 Nginx1.nginx有哪些模块？你比较熟悉哪个？ 2.proxy_cache你是怎么配置的？缓存是存在哪里？具体是怎么命中缓存的？ 简历里有写nginx，结果问得几个问题我都没答好，面试官就没再多问了，囧～ Linux1.怎么查看某个进程中的线程？ 2.怎么批量替换一个文件夹下所有文件中的一个字符？（sed命令） 3.有没有用过jps jmap jstack jstat 命令，分别说下有哪些常用参数，知道多少就说多少。 我这里结合自己用jmap jstack定位到线上问题的经验说的，答完后感觉面试官挺满意的，所以说实践很重要啊～ 情景模拟&amp;其他1.设计一个系统，每天有100亿条数据，需要在后台做实时展示和查找。 我当时回答的大体思路是nginx负载均衡，消息队列存储，多线程读取，批量插入，数据库分库分表。 面试官根据我的回答又衍生出了很多问题，如消息队列存满了怎么办？（也就是消费跟不上生产）批量插入时某一条失败了有什么影响？怎么解决？分库分表应该怎么分？怎么解决数据迁移的问题？ 2.用代码实现cat 1.log |grep a |sort |uniq -c |sort -rn 的功能。 3.如果现在有一台服务器突然变得很慢，怎么去定位问题？ ##感悟 1.不要妄自菲薄。很多人觉得自己学校不好专业不对口，进不了大公司，连去面试的勇气都没有，其实越是大公司越是看重个人能力而不是历史战绩。 2.面试前一定要准备充足，不然就是浪费双方时间。很多人不屑于临时抱佛脚，你如果觉得这是临时抱佛脚那就是抬杠了。我这里说的准备主要有两点：第一，简历上的内容一定都是自己很熟的东西，面试官会根据他感兴趣的地方衍生出很多问题，可能问到的点都提前想一下。第二，一定要有一两个自己很熟悉的领域，可以具体到某个技术点或者某个框架，但是一定要研究透彻。 3.要善于思考、总结、反思，这些能力是可以训练的。 ##写在最后这里只列出了网易面试中问到的问题（其他公司问得也都差不多），问题带答案面经：阿里面经 丨网易面经]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unlink of file 'xx' failed. Should I try again? (y/n) 解决办法]]></title>
    <url>%2F2017%2F08%2F02%2Funlinkerr%2F</url>
    <content type="text"><![CDATA[1Unlink of file &apos;xx&apos; failed. Should I try again? (y/n) 原因：一般遇到这个错输入y/n都不能解决问题，出现这个问题的原因可能是其他程序正在操作git目录下面的文件，导致git无法关联该文件。 比如用dos命令窗或者git bash打开当前分支的文件，不关闭的情况下再切换到其他分支，等再切回来的时候就会报这个错，怎么确认(y)都无济于事。 解决方法：这里的 Unlink of file ‘xx’ failed. 这个 xx 就是被占用的文件，只要找到占用这个文件的程序或进程，并把它kill掉就可以了。 如果实在找不到就直接重启电脑…]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring启动加载过程源码分析]]></title>
    <url>%2F2017%2F07%2F31%2FspringStart%2F</url>
    <content type="text"><![CDATA[我们知道启动spring容器两常见的两种方式（其实都是加载spring容器的xml配置文件时启动的）： 在应用程序下加载 1ApplicationContext ctx = new ClassPathXmlApplicationContext("spring-context.xml"); web模式下加载 1234567&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-context.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 可以发现，执行new ClassPathXmlApplicationContext()的时候会打印以下日志： 2017-5-15 12:48:48 org.springframework.context.support.AbstractApplicationContext prepareRefresh即调用AbstractApplicationContext类的prepareRefresh方法，我们去看AbstractApplicationContext类： 通过类图可以发现AbstractApplicationContext是一个抽象类，也属于BeanFactory体系，实现了ApplicationContext，再往下找到他的子类ClassPathXmlApplicationContext，来看它的具体实现 类里有很多重载的构造函数，到最后都是调用这个：1234567891011public ClassPathXmlApplicationContext(String[] paths, Class clazz, ApplicationContext parent) throws BeansException &#123; super(parent); Assert.notNull(paths, "Path array must not be null"); Assert.notNull(clazz, "Class argument must not be null"); this.configResources = new Resource[paths.length]; for (int i = 0; i &lt; paths.length; i++) &#123; this.configResources[i] = new ClassPathResource(paths[i], clazz); &#125; //IOC初始化过程 refresh();&#125; 这里就到重点了，refresh方法定义了ioc容器启动的整个过程，来看源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 1.Prepare this context for refreshing. prepareRefresh(); // 2.Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 3.Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // 4.Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // 5.Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // 6.Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // 7.Initialize message source for this context. initMessageSource(); // 8.Initialize event multicaster for this context. initApplicationEventMulticaster(); // 9.Initialize other special beans in specific context subclasses. onRefresh(); // 10.Check for listener beans and register them. registerListeners(); // 11.Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // 12.Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; // 13.Destroy already created singletons to avoid dangling resources. destroyBeans(); // 14.Reset 'active' flag. cancelRefresh(ex); // 15.Propagate exception to caller. throw ex; &#125; &#125;&#125; 接下来，一步一步分析spring干了哪些事 1．初始化BeanFactory：根据配置文件实例化BeanFactory，getBeanFactory()方法由具体子类实现。在这一步里，Spring将配置文件的信息解析成为一个个的BeanDefinition对象并装入到容器的Bean定义注册表（BeanDefinitionRegistry）中，但此时Bean还未初始化；obtainFreshBeanFactory()会调用自身的refreshBeanFactory(),而refreshBeanFactory()方法由子类AbstractRefreshableApplicationContext实现，该方法返回了一个创建的DefaultListableBeanFactory对象，这个对象就是由ApplicationContext管理的BeanFactory容器对象。这一步的操作相当于，如果我们在自己的应用代码中不用ApplicationContext而直接用BeanFactory时创建BeanFactory对象的操作，核心代码如下： reader.loadBeanDefinitions(configLocations);2．调用工厂后处理器，根据反射机制从BeanDefinitionRegistry中找出所有BeanFactoryPostProcessor类型的Bean，并调用其postProcessBeanFactory()接口方法。经过第一步加载配置文件，已经把配置文件中定义的所有bean装载到BeanDefinitionRegistry这个Beanfactory中，对于ApplicationContext应用来说这个BeanDefinitionRegistry类型的BeanFactory就是Spring默认的DefaultListableBeanFactory public class DefaultListableBeanFactory extends AbstractAutowireCapableBeanFactory implements ConfigurableListableBeanFactory, BeanDefinitionRegistry{}在这些被装载的bean中，若有类型为BeanFactoryPostProcessor的bean（配置文件中配置的），则将对应的BeanDefinition生成BeanFactoryPostProcessor对象容器扫描BeanDefinitionRegistry中的BeanDefinition，使用java反射自动识别出Bean工厂后处理器（实现BeanFactoryPostProcessor接口）的bean，然后调用这些bean工厂后处理器对BeanDefinitionRegistry中的BeanDefinition进行加工处理，可以完成以下两项工作(当然也可以有其他的操作，用户自己定义)： 1).对使用到占位符的元素标签进行解析，得到最终的配置值，这意味着对一些半成品式的BeanDefinition对象进行加工处理并取得成品的BeanDefinition对象。 2).对BeanDefinitionRegistry中的BeanDefinition进行扫描，通过Java反射机制找出所有属性编辑器的Bean（实现java.beans.PropertyEditor接口的Bean），并自动将它们注册到Spring容器的属性编辑器注册表中（PropertyEditorRegistry），这个Spring提供了实现：CustomEditorConfigurer，它实现了BeanFactoryPostProcessor，用它来在此注册自定义属性编辑器，核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; // Invoke factory processors registered with the context instance. for (Iterator it = getBeanFactoryPostProcessors().iterator(); it.hasNext();) &#123; BeanFactoryPostProcessor factoryProcessor = (BeanFactoryPostProcessor) it.next(); factoryProcessor.postProcessBeanFactory(beanFactory); &#125; // Do not initialize FactoryBeans here: We need to leave all regular beans // 通过ApplicatinContext管理的beanfactory获取已经注册的BeanFactoryPostProcessor类型的bean的名字 String[] factoryProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement the Ordered // interface and those that do not. List orderedFactoryProcessors = new ArrayList(); List nonOrderedFactoryProcessorNames = new ArrayList(); for (int i = 0; i &lt; factoryProcessorNames.length; i++) &#123; if (isTypeMatch(factoryProcessorNames[i], Ordered.class)) &#123; // 调用beanfactory的getBean取得所有的BeanFactoryPostProcessor对象 orderedFactoryProcessors.add(beanFactory.getBean(factoryProcessorNames[i])); &#125; else &#123; nonOrderedFactoryProcessorNames.add(factoryProcessorNames[i]); &#125; &#125; // First, invoke the BeanFactoryPostProcessors that implement Ordered. Collections.sort(orderedFactoryProcessors, new OrderComparator()); for (Iterator it = orderedFactoryProcessors.iterator(); it.hasNext();) &#123; BeanFactoryPostProcessor factoryProcessor = (BeanFactoryPostProcessor) it.next(); // 执行BeanFactoryPostProcessor的方法，传入当前持有的beanfactory对象，以获取要操作的 // BeanDefinition factoryProcessor.postProcessBeanFactory(beanFactory); &#125; // Second, invoke all other BeanFactoryPostProcessors, one by one. for (Iterator it = nonOrderedFactoryProcessorNames.iterator(); it.hasNext();) &#123; String factoryProcessorName = (String) it.next(); ((BeanFactoryPostProcessor) getBean(factoryProcessorName)). postProcessBeanFactory(beanFactory); &#125; &#125;``` BeanFactoryPostProcessor接口代码如下，实际的操作由用户扩展并配置：```javapublic interface BeanFactoryPostProcessor &#123; /** * Modify the application context's internal bean factory after its standard */ void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; &#125; 3．注册Bean后处理器，根据反射机制从BeanDefinitionRegistry中找出所有BeanPostProcessor类型的Bean，并将它们注册到容器Bean后处理器的注册表中，AbstractApplicatinContext中对应代码如下： 1234567891011121314151617181920212223242526272829303132333435protected void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; String[] processorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // Register BeanPostProcessorChecker that logs an info message when int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + processorNames.length; beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); List orderedProcessors = new ArrayList(); List nonOrderedProcessorNames = new ArrayList(); for (int i = 0; i &lt; processorNames.length; i++) &#123; if (isTypeMatch(processorNames[i], Ordered.class)) &#123; orderedProcessors.add(getBean(processorNames[i])); &#125; else &#123; nonOrderedProcessorNames.add(processorNames[i]); &#125; &#125; // First, register the BeanPostProcessors that implement Ordered. Collections.sort(orderedProcessors, new OrderComparator()); for (Iterator it = orderedProcessors.iterator(); it.hasNext();) &#123; // 注册bean后处理器，该方法定义于ConfigurableBeanFactory接口 beanFactory.addBeanPostProcessor((BeanPostProcessor) it.next()); &#125; // Second, register all other BeanPostProcessors, one by one. for (Iterator it = nonOrderedProcessorNames.iterator(); it.hasNext();) &#123; String processorName = (String) it.next(); beanFactory.addBeanPostProcessor((BeanPostProcessor) getBean(processorName)); &#125; &#125; 整段代码类似于第三步的调用工厂后处理器，区别之处在于，工厂后处理器在获取后立即调用，而Bean后处理器在获取后注册到上下文持有的beanfactory中，供以后操作调用（在用户获取bean的过程中，对已经完成属性设置工作的Bean进行后续加工，他加工的是bean，而工厂后处理器加工的是BeanDefinition）BeanPostProcessor 接口代码如下，实际的操作由用户扩展并配置： 1234public interface BeanPostProcessor &#123; Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 4．初始化消息源，初始化容器的国际化信息资源，源代码如下： 123protected void initMessageSource() &#123; // 具体实现&#125; 5．初始化应用上下文事件广播器；（观察者模式中的具体主题角色，持有观察者角色的集合，称为注册表）AbstractApplciationContext拥有一个applicationEventMulticaster 成员变量，applicationEventMulticaster 提供了容器监听器的注册表，成其为事件广播器。在第七步中将会将事件监听器装入其中，AbstractApplicationContext中的代码如下： 1234567891011121314151617181920protected void initApplicationEventMulticaster() &#123; // "applicationEventMulticaster"，先看配置文件中有无配置该类型类（用户扩展 扩展点，如何扩展） if (containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) &#123; this.applicationEventMulticaster = (ApplicationEventMulticaster) getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); &#125; else &#123; // 若没有，则应用Spring框架提供的事件广播器实例 this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(); &#125; &#125; public boolean containsLocalBean(String name) &#123; return getBeanFactory().containsLocalBean(name); &#125; public Object getBean(String name, Class requiredType) throws BeansException &#123; return getBeanFactory().getBean(name, requiredType); &#125; Spring初始化事件广播器，用户可以在配置文件中为容器定义一个自定义的事件广播器（bean的名称要为”applicationEventMulticaster”），只要实现ApplicationEventMulticaster就可以了，Spring在此会根据beanfactory自动获取。如果没有找到外部配置的事件广播器，Spring使用SimpleApplicationEventMulticaster作为事件广播器。 6．初始化其他特殊的Bean：这是一个钩子方法，子类可以借助这个钩子方法执行一些特殊的操作，如AbstractRefreshableWebApplicationContext就使用该钩子方法执行初始化ThemeSource的操作； 123protected void onRefresh() throws BeansException &#123; // For subclasses: do nothing by default.&#125; 7．注册事件监听器；（观察者模式中的观察者角色） Spring根据上下文持有的beanfactory对象，从它的BeanDefinitionRegistry中找出所有实现org.springfamework.context.ApplicationListener的bean，将BeanDefinition对象生成bean，注册为容器的事件监听器，实际的操作就是将其添加到事件广播器所提供的监听器注册表中 AbstractApplicationContext中的代码如下： 123456789101112131415161718192021222324252627282930private List applicationListeners = new ArrayList();public List getApplicationListeners() &#123; return this.applicationListeners;&#125;protected void registerListeners() &#123; // Register statically specified listeners first. for (Iterator it = getApplicationListeners().iterator(); it.hasNext();) &#123; addListener((ApplicationListener) it.next()); &#125; // 获取ApplicationListener类型的所有bean，即事件监听器 // uninitialized to let post-processors apply to them! Collection listenerBeans = getBeansOfType(ApplicationListener.class, true, false).values(); for (Iterator it = listenerBeans.iterator(); it.hasNext();) &#123; // 将事件监听器装入第五步初始化的事件广播器 addListener((ApplicationListener) it.next()); &#125;&#125;public Map getBeansOfType(Class type, boolean includePrototypes, boolean allowEagerInit) throws BeansException &#123; return getBeanFactory().getBeansOfType(type, includePrototypes, allowEagerInit);&#125;protected void addListener(ApplicationListener listener) &#123; getApplicationEventMulticaster().addApplicationListener(listener);&#125; ApplicationListener 的源代码如下：12345678/** * Interface to be implemented by application event listeners. * @see org.springframework.context.event.ApplicationEventMulticaster */ public interface ApplicationListener extends EventListener &#123; void onApplicationEvent(ApplicationEvent event); &#125; 8．初始化singleton的Bean：实例化所有singleton的Bean，并将它们放入Spring容器的缓存中；这就是和直接在应用中使用BeanFactory的区别之处，在创建ApplicationContext对象时，不仅创建了一个BeanFactory对象，并且还应用它实例化所有单实例的bean。（在spring的配置文件中，bean默认为单例，除非在bean的配置中显式指定scope=”prototype”） AbstractApplicationContext中的代码如下： 1beanFactory.preInstantiateSingletons(); 9．发布上下文刷新事件：在此处时容器已经启动完成，发布容器refresh事件（ContextRefreshedEvent） 创建上下文刷新事件，事件广播器负责将些事件广播到每个注册的事件监听器中。 1234567891011publishEvent(new ContextRefreshedEvent(this)); public void publishEvent(ApplicationEvent event) &#123; Assert.notNull(event, "Event must not be null"); // 在此获取事件广播器，并调用其方法发布事件：调用所有注册的监听器的方法 getApplicationEventMulticaster().multicastEvent(event); if (this.parent != null) &#123; this.parent.publishEvent(event); &#125; &#125; 至此，ApplicationContext对象就完成了初始化工作：创建BeanFactory来装配BeanDefiniton，加工处理BeanDefiniton，注册了bean后处理器，初始化了消息资源，初始化了应用上下文事件广播器，注册了事件监听器，初始化了所有singleton的bean，最后发布上下文刷新事件。]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring实现定时任务]]></title>
    <url>%2F2017%2F03%2F15%2FspringTask%2F</url>
    <content type="text"><![CDATA[今天在项目里需要实现一个定时任务，每隔3个小时将过滤的广告通过邮件上报给运营一次。考虑了一下，从实现的技术上可以有三种做法： Java自带的java.util.Timer类，这个类允许调度一个java.util.TimerTask任务。使用这种方式可以让你的程序按照某一个频度执行，但不能在指定时间运行。 使用Quartz，这是一个功能比较强大的的调度器，可以让程序在指定时间执行，也可以按照某一个频度执行，不过配置起来稍显复杂。 Spring3.0以后自带的task，其实就是一个轻量级的Quartz，而且使用起来比Quartz简单许多。支持注解和配置文件两种方式。 综合考虑，最后使用第三种方案，在这里记录一下：首先在spring的配置文件头中配置命名空间及描述123&lt;beans xmlns="http://www.springframework.org/schema/beans"xmlns:task="http://www.springframework.org/schema/task"xsi:schemaLcation="http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.0.xsd"&gt; 然后配置task任务扫描注解，可以理解为定时器开关1&lt;task:annotation-driven/&gt; 最后配置扫描位置：1&lt;context:component-scan base-package=&quot;com.test&quot;/&gt; java代码：123456789101112package com.test;public interface TestService &#123; public void test(); &#125; @Component //import org.springframework.stereotype.Component; public class MyTestServiceImpl implements TestService &#123; @Scheduled(cron="0 0/5 * * * ?") //每5分钟执行一次 @Override public void test()&#123; System.out.println("定时任务执行..."); &#125; &#125; 执行后控制台每5分钟就会打印出：定时任务执行… 了这里有两个地方需要注意：1.定时器方法所在的类上要配置注解@Component,而不是接口2.除了上边用到的这种通过@Scheduled注解外还可以通过配置文件实现123&lt;task:scheduled-tasks&gt; &lt;task:scheduled ref=&quot;testTask&quot; method=&quot;test&quot; cron=&quot;0 0/5 * * * ?&quot;/&gt; &lt;/task:scheduled-tasks&gt; 这里cron的值是通过一种cron表达式来配置的，Cron表达式是一个字符串，字符串以5或6个空格隔开，分为6或7个域，每一个域代表一个含义，Cron有如下两种语法格式： Seconds Minutes Hours DayofMonth Month DayofWeek Year Seconds Minutes Hours DayofMonth Month DayofWeek每一个域可出现的字符如下： Seconds:可出现”, - * /“四个字符，有效范围为0-59的整数 Minutes:可出现”, - * /“四个字符，有效范围为0-59的整数 Hours:可出现”, - * /“四个字符，有效范围为0-23的整数 DayofMonth:可出现”, - * / ? L W C”八个字符，有效范围为0-31的整数 Month:可出现”, - * /“四个字符，有效范围为1-12的整数或JAN-DEc DayofWeek:可出现”, - * / ? L C #”四个字符，有效范围为1-7的整数或SUN-SAT两个范围。1表示星期天，2表示星期一， 依次类推 Year:可出现”, - * /“四个字符，有效范围为1970-2099年每一个域都使用数字，但还可以出现如下特殊字符，它们的含义是： * : 表示匹配该域的任意值，假如在Minutes域使用*, 即表示每分钟都会触发事件 ? : 只能用在DayofMonth和DayofWeek两个域。它也匹配域的任意值，但实际不会。因为DayofMonth和 DayofWeek会相互影响。例如想在每月的20日触发调度，不管20日到底是星期几，则只能使用如下写法：13 13 15 20 ?, 其中最后一位只能用？，而不能使用，如果使用*表示不管星期几都会触发，实际上并不是这样 - : 表示范围，例如在Minutes域使用5-20，表示从5分到20分钟每分钟触发一次 / : 表示起始时间开始触发，然后每隔固定时间触发一次，例如在Minutes域使用5/20,则意味着5分钟触发一次，而25，45等分别触发一次 , : 表示列出枚举值值。例如：在Minutes域使用5,20，则意味着在5和20分每分钟触发一次 L : 表示最后，只能出现在DayofWeek和DayofMonth域，如果在DayofWeek域使用5L,意味着在最后的一个星期四触发 W : 表示有效工作日(周一到周五),只能出现在DayofMonth域，系统将在离指定日期的最近的有效工作日触发事件。例如：在DayofMonth使用5W，如果5日是星期六，则将在最近的工作日：星期五，即4日触发。如果5日是星期天，则在6日(周一)触发；如果5日在星LW:这两个字符可以连用，表示在某个月最后一个工作日，即最后一个星期五 # : 用于确定每个月第几个星期几，只能出现在DayofMonth域。例如在4#2，表示某月的第二个星期三]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guavacache 缓存策略选择]]></title>
    <url>%2F2017%2F03%2F07%2Fguavacache%2F</url>
    <content type="text"><![CDATA[三个参数 expireAfterAccess: 当缓存项在指定的时间段内没有被读或写就会被回收。 expireAfterWrite：当缓存项在指定的时间段内没有更新就会被回收。 refreshAfterWrite：当缓存项上一次更新操作之后的多久会被刷新。 expireAfterWrite 使每次更新之后的指定时间让缓存失效，然后重新加载缓存。guava cache 会严格限制只有1个加载操作，这样会很好地防止缓存失效的瞬间大量请求穿透到后端引起雪崩效应。 通过分析源码，guava cache 在限制只有1个加载操作时进行加锁，其他请求必须阻塞等待这个加载操作完成；而且，在加载完成之后，其他请求的线程会逐一获得锁，去判断是否已被加载完成，每个线程必须轮流地走一个“”获得锁，获得值，释放锁“”的过程，这样性能会有一些损耗。 refreshAfterWrite 在refresh的过程中，严格限制只有1个重新加载操作，而其他查询先返回旧值，这样有效地可以减少等待和锁争用。 但它也有一个缺点，因为到达指定时间后，它不能严格保证所有的查询都获取到新值。 guava cache 并没使用额外的线程去做定时清理和加载的功能，而是依赖于查询请求。在查询的时候去比对上次更新的时间，如超过指定时间则进行加载或刷新。 所以，如果使用refreshAfterWrite，在吞吐量很低的情况下，如很长一段时间内没有查询之后，发生的查询有可能会得到一个旧值（这个旧值可能来自于很长时间之前）。 可以看出 refreshAfterWrite 和 expireAfterWrite 两种方式各有优缺点，各有使用场景。那么能否在 refreshAfterWrite 和 expireAfterWrite 找到一个折中？ 比如说控制缓存每1s进行 refresh，如果超过2s没有访问，那么则让缓存失效，下次访问时不会得到旧值，而是必须得待新值加载。 google 到这种方案亲测有效： 1CacheBuilder.newBuilder().maximumSize(1000).refreshAfterWrite(1, TimeUnit.SECONDS).expireAfterWrite(2, TimeUnit.SECONDS).build();]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jetty.xml解析]]></title>
    <url>%2F2017%2F03%2F07%2FjettyXml%2F</url>
    <content type="text"><![CDATA[jetty有一种启动方式是在jetty的根目录中运行命令行：java -jar start.jar，这个命令会调用apache的XmlConfiguration工具类作为启动类，这个类会默认读取/etc/jetty.xml文件，加载jetty启动所必须的配置。接下来分段研究：1&lt;Configure id=&quot;Server&quot; class=&quot;org.eclipse.jetty.server.Server&quot;&gt; 这段配置是整个配置文件的root元素，它其实是调用org.eclipse.jetty.server类的默认构造函数来创建一个server对象，对应源码：123public Server()&#123; this((ThreadPool)null);&#125; 创建了一个空的server，那么接下来就是为这个server设置各种对象及属性了。继续看：1234567&lt;Set name=&quot;ThreadPool&quot;&gt; &lt;New class=&quot;org.eclipse.jetty.util.thread.QueuedThreadPool&quot;&gt; &lt;Set name=&quot;minThreads&quot;&gt;100&lt;/Set&gt; &lt;Set name=&quot;maxThreads&quot;&gt;1000&lt;/Set&gt; &lt;Set name=&quot;SpawnOrShrinkAt&quot;&gt;2&lt;/Set&gt; &lt;/New&gt;&lt;/Set&gt; 这段配置是为当前server设置线程池，从xml元素也可以看出来它其实是调用server的SetThreadPool方法：1server.setThreadPool(threadPool); 然后new了一个org.eclipse.jetty.util.thread.QueuedThreadPool对象，并设置它的最小线程数、最大线程数以及最多有多少task可以暂时放到队列里待会执行，还有其他很多参数具体场景可能需要不同的配置。再继续看：1234567891011121314&lt;Call name=&quot;addConnector&quot;&gt; &lt;Arg&gt; &lt;New class=&quot;org.eclipse.jetty.server.nio.SelectChannelConnector&quot;&gt; &lt;Set name=&quot;host&quot;&gt;&lt;Property name=&quot;jetty.host&quot; /&gt;&lt;/Set&gt; &lt;Set name=&quot;port&quot;&gt;&lt;Property name=&quot;jetty.port&quot; default=&quot;8080&quot;/&gt;&lt;/Set&gt; &lt;Set name=&quot;maxIdleTime&quot;&gt;3000&lt;/Set&gt; &lt;Set name=&quot;Acceptors&quot;&gt;2&lt;/Set&gt; &lt;Set name=&quot;statsOn&quot;&gt;false&lt;/Set&gt; &lt;Set name=&quot;confidentialPort&quot;&gt;9043&lt;/Set&gt; &lt;Set name=&quot;lowResourcesConnections&quot;&gt;20000&lt;/Set&gt; &lt;Set name=&quot;lowResourcesMaxIdleTime&quot;&gt;10000&lt;/Set&gt; &lt;/New&gt; &lt;/Arg&gt;&lt;/Call&gt; 接下来这一段配置是调用addConnector方法，为当前的server对象添加一个connector，即监听器，jetty默认情况下创建的就是SelectChannelConnector，也就是所谓的nioConnector，关于bio,nio和nio的区别网上资料很多可自行百度。这个方法传入了很多参数，host、端口port、连接最大的空闲时间maxIdleTime、创建selector个数Acceptors等等，后面这些参数实际项目配置的都不同，但是都会有一个经验值。关于SelectChannelConnector的详细介绍参考这里： http://blog.csdn.net/kobejayandy/article/details/20166215 再继续看：1234567891011121314&lt;Set name=&quot;handler&quot;&gt; &lt;New id=&quot;Handlers&quot; class=&quot;org.eclipse.jetty.server.handler.HandlerCollection&quot;&gt; &lt;Set name=&quot;handlers&quot;&gt; &lt;Array type=&quot;org.eclipse.jetty.server.Handler&quot;&gt; &lt;Item&gt; &lt;New id=&quot;Contexts&quot; class=&quot;org.eclipse.jetty.server.handler.ContextHandlerCollection&quot;/&gt; &lt;/Item&gt; &lt;Item&gt; &lt;New id=&quot;DefaultHandler&quot; class=&quot;org.eclipse.jetty.server.handler.DefaultHandler&quot;/&gt; &lt;/Item&gt; &lt;/Array&gt; &lt;/Set&gt; &lt;/New&gt;&lt;/Set&gt; 这段配置是去调用server的setHandler方法为当前的server设置handler属性，这里默认创建的是org.mortbay.jetty.handler.HandlerCollection，接着给这个handlerCollection创建了两个handler分别是ContextHandlerCollection和DefaultHandler。我们知道当jetty收到一个http请求之后，会调用server来处理这个请求，而server对象是调用内部的handler来处理，所以关键还是看HandlerCollection是怎么处理的吧，也就是它的handle方法，源码：123456789101112131415161718192021222324252627282930313233343536373839@Overridepublic void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException &#123; if (_handlers!=null &amp;&amp; isStarted()) &#123; MultiException mex=null; for (int i=0;i&lt;_handlers.length;i++) &#123; try &#123; _handlers[i].handle(target,baseRequest, request, response); &#125; catch(IOException e) &#123; throw e; &#125; catch(RuntimeException e) &#123; throw e; &#125; catch(Exception e) &#123; if (mex==null) mex=new MultiException(); mex.add(e); &#125; &#125; if (mex!=null) &#123; if (mex.size()==1) throw new ServletException(mex.getThrowable(0)); else throw new ServletException(mex); &#125; &#125;&#125; 代码其实很简单，主要是那个for循环遍历当前collection里所有的handler，依次调用它们的handle方法。所以我们平常项目里有些初始化操作会写在handler类里边在jetty启动的时候就会执行初始化。 好了，到这里整个jetty.xml的文件最重要的一些内容就配置完了，接下来还有一些其他的配置，一般情况下用默认值就好了。]]></content>
      <categories>
        <category>Java</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Jetty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常用统计命令]]></title>
    <url>%2F2017%2F02%2F20%2FgitCommands%2F</url>
    <content type="text"><![CDATA[上周要做个汇报PPT涉及到个人对项目贡献量，在网上搜集了些常用统计命令，总结如下： 统计代码提交量git log –author=”$(gitconfig–getuser.name)” –pretty=tformat: –numstat | gawk ‘{add += $1;subs += $2;loc += $1 - $2} END {printf “added lines:%s removed lines : %s total lines: %s\n”,add,subs,loc}’ - 统计提交量排名git log –pretty=’%aN’ | sort | uniq-c | sort -k1 -n -r git log参数说明： –author：指定作者 –stat：显示每次更新的文件修改统计信息，会列出具体文件列表 –shortstat：统计每个commit的文件修改行数，包括添加、删除，但不列出文件列表 –numstat：统计每个commit的文件修改行数，包括添加、删除，并列出文件列表 -p：选项展开显示每次提交的内容差异，用-2则仅显示最近的两次更新 –name-only：仅在提交信息后显示已修改的文件清单 –name-status：显示添加、修改、删除的文件清单 –graph：显示ASCII图形表示的分支合并历史 –pretty：使用其他格式显示历史提交信息。可用的选项包括oneline、short、full、fuller和format –pretty=tformat: 可以定制要显示的记录格式，这样的输出便于后期编程提取分析]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 4种对象的理解]]></title>
    <url>%2F2017%2F02%2F19%2FgitObject%2F</url>
    <content type="text"><![CDATA[Git中有四种基本对象类型，可以说Git的所有操作都是通过这四种对象完成的。 blob对象。blob文件是一种二进制文件，当把一个文件add进暂存区时就生成一个blob对象，它包含了这个文件的所有数据，但不包括文件名字、路径、格式等信息，之后每次提交只要文件有变更都会生成一个新的blob对象，但原有的blob对象也会保存下来，也就是说只要内容相同的文件在Objects库中只存在一份，这也就是Git和其他版本控制系统的区别。还要说明的是，每个blob对象的id是对文件内容进行hash得到的，这样在比较文件是否修改了的时候只用比较对应的blob对象名是否相同就可以了。 tree对象。可以把它想象成文件目录，每个tree对象包含0个或多个tree对象和blob对象。 commit对象。每次提交都会生成一个commit对象，而每个commit对象都对应一个tree对象，通过这个tree对象可以查看这次提交的所有信息。 tags对象。tags对象主要是为了解决每次提交id太长不好记的问题，可以对每次提交标注一个tag。顺便吐槽下,《Git版本控制管理》中文第二版真的翻译的一般…]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git合并分支命令参数详解：git merge --ff]]></title>
    <url>%2F2017%2F02%2F16%2FgitMerge%2F</url>
    <content type="text"><![CDATA[今天研究了一下git merge命令常用参数，并分别用简单的例子实验了一下，整理如下： 相关参数 –ff 快速合并，这个是默认的参数。如果合并过程出现冲突，Git会显示出冲突并等待手动解决 –ff-only 只有能快速合并的情况才合并。如果合并过程出现冲突，Git会自动abort此次merge –no-ff 不使用快速合并。会生成一次新的提交记录，这个记录只是标识在这里进行了一次merge操作（目前还没想到应用场景） –squash 压缩合并。将待合并的分支的内容压缩成一个新的提交合并进来 应用场景备注：C代表一次提交，合并时都是将dev分支合并到master。 第一种情况：master分支切出dev分支后没有新的提交，也就是说只有dev分支有更新，可以快速合并的情况：eg：master：C1 ← C2 ↑ dev： C3 ← C4 执行：git merge –ff dev master：C1 ← C2 ← C3 ← C4 dev：C1 ← C2 ← C3 ←C4 结果：查看git log时master分支会看到dev分支上的所有提交，此时master和dev是一样的 执行：git merge –ff-only dev 结果同上。 执行：git merge –no-ff dev git会提示让你输入此次合并的信息，然后生成一个特殊的commit。 master:C1 ← C2 ← C3 ← C4 ← C5 (Merge branch ‘dev’) dev：C1 ← C2 ← C3 ←C4 结果：master分支会比dev分支多一条提交记录，也就是刚才输入犯人合并信息 执行：git merge –squash dev master：C1 ← C2 ← C5 (Merge branch ‘dev’) dev：C1 ← C2 ← C3 ←C4 结果：这里的C5其实是C3和C4的合并，如果只想合并dev的内容但是不需要它的提交记录就可以用这个参数 第二种情况，切出后master和dev分支均有更新，这种情况是最常见的。这里为了演示冲突，在C4和C5分别对一个文件进行了修改。eg：master：C1 ← C2 ← C4 ↑ dev： C3 ← C5 执行：git merge –ff dev 这时Git会告诉你产生了冲突并列出冲突的文件，查看文件时会列出具体冲突内容，这时要先解决冲突（如果使用Intellij Idea或Eclipse等工具，可以直接选择use ours/theirs，ours代表被合并分支即master，theirs代表合并分支即dev），然后将这些修改的部分提交，再执行merge操作。 master：C1 ← C2 ← C3 ← C5 ← C4 ← C6 (解决冲突的那次提交) dev：C1 ← C2 ← C3 ←C5 那么问题来了，Git是如何知道两个文件有冲突呢？ 这里先说下结论，有时间再补一篇文章单独说明说明。 大家都知道在Git里每个文件都是一个blob对象，这里先不管合并时怎么找到同一个文件在两个分支上的blob（其实如果文件没有更新，在两个分支上是指向同一个blob），假设现在已经到了比较阶段了，Git会拿两个文件来逐行进行对比，但是判定是否修改是通过相邻行来确定的。也就是说文件a的第三行修改了，Git是通过第2行和第4行的对比来判定的，不信的可以先自己做实验验证。由于篇幅原因，这里不再赘述。 执行：git merge –ff-only dev 这时Git会检测到产生了冲突，所以提示：Not possible to fast-forward, aborting. 即取消这次merge操作。 执行：git merge –no-ff dev 结果同1，不过这里在解决了冲突执行commit操作后不用再进行merge操作了。如果再执行merge操作，它会提示：Already up-to-date. 执行：git merge –squash dev master：C1 ← C2 ← C4 ← C6 (解决冲突的那次提交) dev：C1 ← C2 ← C3 ←C5 这里解决了冲突并提交之后也不用再执行merge操作了。如果再执行merge操作会有两种情况： a.刚才解决冲突时选用了master分支的修改，那么还是会提示有冲突需要解决。 b.刚才解决冲突时选用了dev分支的修改，那么会提示Already up-to-date。 对比发现，使用–squash参数时，如果有冲突，解决完冲突后只要两个分支不完全一样，再执行git merge –squash时还是会进行merge。但–no-ff就不会。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
